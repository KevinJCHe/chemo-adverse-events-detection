#!/usr/bin/env python
# coding: utf-8
"""
========================================================================
Â© 2021 Institute for Clinical Evaluative Sciences. All rights reserved.

TERMS OF USE:
##Not for distribution.## This code and data is provided to the user solely for its own non-commercial use by individuals and/or not-for-profit corporations. User shall not distribute without express written permission from the Institute for Clinical Evaluative Sciences.

##Not-for-profit.## This code and data may not be used in connection with profit generating activities.

##No liability.## The Institute for Clinical Evaluative Sciences makes no warranty or representation regarding the fitness, quality or reliability of this code and data.

##No Support.## The Institute for Clinical Evaluative Sciences will not provide any technological, educational or informational support in connection with the use of this code and data.

##Warning.## By receiving this code and data, user accepts these terms, and uses the code and data, solely at its own risk.
========================================================================
"""
# In[1]:


import sys


# In[2]:


for i, p in enumerate(sys.path):
    sys.path[i] = sys.path[i].replace("/software/anaconda/3/", "/MY/DATA/.conda/envs/myenv/")
sys.prefix = '/MY/DATA/.conda/envs/myenv/'


# In[3]:


import os
import tqdm
import pandas as pd
import numpy as np
import utilities as util
import warnings
import pickle
# warnings.filterwarnings('ignore')

import matplotlib.pyplot as plt
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, GroupShuffleSplit
from sklearn.linear_model import Ridge, LogisticRegression
from sklearn.metrics import (mean_squared_error,
                             classification_report, accuracy_score,
                            plot_confusion_matrix, confusion_matrix, 
                            plot_roc_curve, roc_auc_score, roc_curve, 
                            average_precision_score, plot_precision_recall_curve, precision_recall_curve)
from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier
from sklearn.neural_network import MLPRegressor, MLPClassifier
from sklearn.multioutput import MultiOutputRegressor, MultiOutputClassifier
from sklearn.calibration import CalibratedClassifierCV, calibration_curve
from xgboost import XGBRegressor, XGBClassifier

from bayes_opt import BayesianOptimization

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.utils.data as Data
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader, TensorDataset
from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence


# In[4]:


df = util.read_partially_reviewed_csv()
df = util.get_included_regimen(df)
cycle_lengths = df['cycle_length'].to_dict()
del df

esas_ecog_features = pd.read_csv('data/chemo_processed2.csv')
esas_ecog_features = esas_ecog_features[['Anxiety', 'Lack of Appetite', 'Depression', 'Nausea', 'Tiredness', 'Pain', 
                                         'Drowsiness', 'Wellbeing', 'Shortness of Breath', 'ecog_grade']]

blood_types = ['neutrophil', 'hemoglobin', 'platelet']


# In[5]:


def read_data(blood_type):
    df = pd.read_csv(f'data/{blood_type}.csv')
    # include the new questionnaire data features
    df = pd.concat([df, esas_ecog_features], axis=1)
    # turn string of numbers columns into integer column 
    df = df.rename({str(i): i for i in range(-5, 29)}, axis='columns')
    # exclude some regimens
    exclude_regimen = ['ac-pacl(dd)', 'fec-d', 'crbpgemc', 'ac-pacl(w)', 'bmp']
    df = df[~df['regimen'].isin(exclude_regimen)]  
    # convert each day into a row
    df = df.melt(id_vars=df.columns[~df.columns.isin(range(-5,29))], var_name='Day', value_name=f'{blood_type}_value')
    df = df[~df[f'{blood_type}_value'].isnull()]
    df = df.sort_values(by=['ikn', 'prev_visit'])
    # remove negative days (they are duplicates of prev blood values mostly)
    df = df[~df['Day'] < 0]
    return df

def get_data():
    data = {blood_type: read_data(blood_type) for blood_type in blood_types}
    # keep only rows where all blood types are present
    n_indices = data['neutrophil'].index
    h_indices = data['hemoglobin'].index
    p_indices = data['platelet'].index
    keep_indices = n_indices[n_indices.isin(h_indices) & n_indices.isin(p_indices)]
    data = {blood_type: data[blood_type].loc[keep_indices] for blood_type in blood_types}
    return data

def organize_data(data):
    model_data = data['neutrophil'] # all blood types have the same values
    model_data['hemoglobin_value'] = data['hemoglobin']['hemoglobin_value']
    model_data['platelet_value'] = data['platelet']['platelet_value']
    model_data['Day'] = model_data['Day'].astype(int)
    
    model_data['prev_visit'] = pd.to_datetime(model_data['prev_visit'])
    target_cols = ['target_neutrophil_value', 'target_hemoglobin_value', 
                   'target_platelet_value', 'target_day_since_starting']
    values = []
    for ikn, group in tqdm.tqdm(model_data.groupby('ikn')):
        start_date = group['prev_visit'].iloc[0]
        group['days_since_starting'] = (group['prev_visit'] - start_date + 
                                        pd.to_timedelta(group['Day'], unit='D')).dt.days
        
        group[target_cols] = group[['neutrophil_value', 'hemoglobin_value', 
                                    'platelet_value', 'days_since_starting']].shift(-1)
        
        # discard last row as there are no target blood counts to predict
        group = group.iloc[:-1]
        
        values.extend(group.values.tolist())
    model_data = pd.DataFrame(values, columns=group.columns)
    return model_data


# In[ ]:


data = get_data()
model_data = organize_data(data.copy())
# model_data.to_csv('models/GRU/model_data.csv', index=False)


# In[6]:


def remove_outliers(data):
    """
    Remove the upper and lower 1 percentiles for the columns indicated below
    """
    cols = ['neutrophil_value', 'hemoglobin_value', 'platelet_value', 
            'target_neutrophil_value', 'target_hemoglobin_value', 'target_platelet_value']
    num_rows_removed = 0
    for col in cols:
        size = len(data)
        percentile1 = data[col].quantile(0.01)
        percentile99 = data[col].quantile(0.99)
        data = data[(data[col] > percentile1) & (data[col] < percentile99)]
        # print(f'Removed outliers from column {col}, {size-len(data)} rows removed')
        num_rows_removed += size-len(data)
    print('Total number of outlier rows removed =', num_rows_removed)
    return data  

def dummify_data(data):
    # make categorical columns into one-hot encoding
    cols = ['regimen','intent_of_systemic_treatment','line_of_therapy', 'lhin_cd', 'curr_morph_cd', 'curr_topog_cd', 
             'sex']
    return pd.get_dummies(data, columns=cols)

def replace_missing_body_surface_area(data, means_train=None):
    # replace missing body surface area with the mean based on sex
    bsa = 'body_surface_area'
    means = {'female': data.loc[data['sex_F'] == 1, bsa].mean(),
             'male': data.loc[data['sex_M'] == 1, bsa].mean()} if means_train is None else means_train
    data.loc[data['sex_F'] == 1, bsa] = data.loc[data['sex_F'] == 1, bsa].fillna(means['female'])
    data.loc[data['sex_M'] == 1, bsa] = data.loc[data['sex_M'] == 1, bsa].fillna(means['male'])
    if means_train is None:
        return data, means
    else:
        return data
    
def split_data(data):
    """
    Split data into training, validation and test sets based on patient ids
    """
    # convert dtype object to float
    data = data[data.columns[~data.columns.isin(['visit_date', 'prev_visit', 'chemo_interval', 'Day'])]]
    data = data.astype(float)
    
    # create training set
    gss = GroupShuffleSplit(n_splits=1, test_size=0.4, random_state=42)
    train_idxs, test_idxs = next(gss.split(data, groups=data['ikn']))
    train_data = data.iloc[train_idxs]
    test_data = data.iloc[test_idxs]

    # crate validation and testing set
    gss = GroupShuffleSplit(n_splits=1, test_size=0.5, random_state=42)
    valid_idxs, test_idxs = next(gss.split(test_data, groups=test_data['ikn']))

    valid_data = test_data.iloc[valid_idxs]
    test_data = test_data.iloc[test_idxs]

    # sanity check - make sure there are no overlap of patients in the splits
    assert(sum(valid_data['ikn'].isin(set(train_data['ikn']))) + sum(valid_data['ikn'].isin(set(test_data['ikn']))) + 
           sum(train_data['ikn'].isin(set(valid_data['ikn']))) + sum(train_data['ikn'].isin(set(test_data['ikn']))) + 
           sum(test_data['ikn'].isin(set(train_data['ikn']))) + sum(test_data['ikn'].isin(set(valid_data['ikn']))) 
           == 0)
    print(f'Size of splits: Train:{len(train_data)}, Val:{len(valid_data)}, Test:{len(test_data)}')
    print(f"Number of patients: Train:{len(set(train_data['ikn']))}, Val:{len(set(valid_data['ikn']))}, Test:{len(set(test_data['ikn']))}")
    
    # replace missing body surface area with the mean
    train_data, means_train = replace_missing_body_surface_area(train_data.copy())
    print(f"Body Surface Area Mean - Female:{means_train['female']}, Male:{means_train['male']}")
    valid_data = replace_missing_body_surface_area(valid_data.copy(), means_train)
    test_data = replace_missing_body_surface_area(test_data.copy(), means_train)
    
    # normalize the splits based on training data
    train_data, minmax_train = normalize_data(train_data.copy())
    valid_data, minmax_valid = normalize_data(valid_data.copy(), minmax_train)
    test_data, minmax_test = normalize_data(test_data.copy(), minmax_train)
    
    # split into input features and target labels
    cols = data.columns
    feature_cols = cols[~cols.str.contains('target')]
    feature_cols = feature_cols.tolist() + ['target_day_since_starting']
    target_cols = cols[cols.str.contains('target')]
    target_cols = target_cols.drop('target_day_since_starting')
    X_train, X_valid, X_test = train_data[feature_cols], valid_data[feature_cols], test_data[feature_cols]
    Y_train, Y_valid, Y_test = train_data[target_cols], valid_data[target_cols], test_data[target_cols]
    
    return [(X_train, Y_train, minmax_train), (X_valid, Y_valid, minmax_valid), (X_test, Y_test, minmax_test)]

def normalize_data(data, minmax_train=None):
    minmax = pd.DataFrame(index=['min', 'max'])
    
    cols = ['chemo_cycle', 'age', 'body_surface_area', 'days_since_starting',
            'target_day_since_starting'] + esas_ecog_features.columns.tolist()
    for col in cols:
        tmp = data[col]
        minmax[col] = [tmp.min(), tmp.max()]
        maximum = minmax.loc['max', col] if minmax_train is None else minmax_train.loc['max', col]
        minimum = minmax.loc['min', col] if minmax_train is None else minmax_train.loc['min', col]
        data[col] = (tmp-minimum) / (maximum-minimum)
    
    for blood_type in blood_types:
        cols = [f'{blood_type}_value', f'target_{blood_type}_value']
        tmp = data[cols]
        minmax[blood_type] = [tmp.min().min(), tmp.max().max()]
        maximum = minmax.loc['max', blood_type] if minmax_train is None else minmax_train.loc['max', blood_type]
        minimum = minmax.loc['min', blood_type] if minmax_train is None else minmax_train.loc['min', blood_type]
        data[cols] = (tmp-minimum) / (maximum-minimum)
    
    return data, minmax


# In[7]:


model_data = pd.read_csv('models/GRU/model_data.csv')
model_data


# In[8]:


model_data = remove_outliers(model_data)
model_data = dummify_data(model_data)
train, valid, test = split_data(model_data)


# In[9]:


X_train, Y_train, minmax = train
X_valid, Y_valid, minmax = valid
X_test, Y_test, minmax = test


# In[10]:


train_mapping = {}
for ikn, group in tqdm.tqdm(X_train.groupby('ikn')):
    group = group.drop(columns=['ikn'])
    train_mapping[ikn] = (group, Y_train.loc[group.index])

valid_mapping = {}
for ikn, group in tqdm.tqdm(X_valid.groupby('ikn')):
    group = group.drop(columns=['ikn'])
    valid_mapping[ikn] = (group, Y_valid.loc[group.index])
    
test_mapping = {}
for ikn, group in tqdm.tqdm(X_test.groupby('ikn')):
    group = group.drop(columns=['ikn'])
    test_mapping[ikn] = (group, Y_test.loc[group.index])


# In[11]:


class seq_data(TensorDataset):
    def __init__(self, mapping, ids):
        self.mapping = mapping
        self.ids = ids
                
    def __getitem__(self, index):
        sample = self.ids[index]
        X, Y = self.mapping[sample]
        features_tensor = torch.Tensor(X.values)
        target_tensor = torch.Tensor(Y.values)
        return features_tensor, target_tensor
    
    def __len__(self):
        return(len(self.ids))


# In[12]:


train_dataset = seq_data(mapping=train_mapping, ids=X_train['ikn'].unique())
valid_dataset = seq_data(mapping=valid_mapping, ids=X_valid['ikn'].unique())
test_dataset = seq_data(mapping=test_mapping, ids=X_test['ikn'].unique())


# In[13]:


class GRU_model(nn.Module):
    def __init__(self, n_features, n_targets, hidden_size, hidden_layers, batch_size, dropout, pad_value):
        super().__init__()
        self.n_features = n_features
        self.n_targets = n_targets
        self.hidden_size = hidden_size
        self.hidden_layers = hidden_layers
        self.batch_size = batch_size
        self.pad_value = pad_value
        self.dropout = dropout
        self.rnn_layers = nn.GRU(input_size=self.n_features, hidden_size=self.hidden_size, 
                                 num_layers=self.hidden_layers, dropout=self.dropout, batch_first=True)
        self.linear = nn.Linear(in_features=self.hidden_size, 
                                out_features=self.n_targets)

    def forward(self, packed_inputs):
        out_packed, _ = self.rnn_layers(packed_inputs)
        output, lengths = pad_packed_sequence(out_packed, batch_first=True, padding_value=self.pad_value)
        output = self.linear(output)
        return output
    
    def init_hidden(self):
        return torch.zeroes(self.hidden_layers, self.batch_size, self.hidden_size)


# # Regression

# In[41]:


def evaluate(model, loader, criterion, pad_value):
    total_loss = 0
    for i, batch in enumerate(loader):
        inputs, targets = tuple(zip(*batch))
        seq_lengths = list(map(len, inputs))
        padded = pad_sequence(inputs, batch_first=True, padding_value=pad_value)
        padded_packed = pack_padded_sequence(padded, seq_lengths, batch_first=True, enforce_sorted=False).float()
        padded_targets = pad_sequence(targets, batch_first=True, padding_value=pad_value)
        targets = torch.cat(targets).float()
        if torch.cuda.is_available():
            padded_packed = padded_packed.cuda()
            targets = targets.cuda()
        preds = model(padded_packed)
        flag = (padded_targets != pad_value)
        preds_res = preds[flag].reshape(-1, 3).split(seq_lengths)
        preds = torch.cat(preds_res).float()
        loss = criterion(preds, targets)
        loss = loss.mean(axis=0)
        total_loss += loss
    return total_loss.cpu().detach().numpy() / (i+1)

def train_regression(batch_size=512, pad_value=-999, epochs=200, learning_rate=0.001, 
                hidden_size=20, hidden_layers=3, dropout=0.5, decay=0, save=False, save_path=None):
    
    n_features = len(X_train.columns) - 1 # -1 for ikn
    model = GRU_model(n_features=n_features, n_targets=3, hidden_size=hidden_size, hidden_layers=hidden_layers,
                      batch_size=batch_size, dropout=dropout, pad_value=pad_value)
    if torch.cuda.is_available():
        model.cuda()

    train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=False, collate_fn=lambda x:x)
    valid_loader = DataLoader(dataset=valid_dataset, batch_size=batch_size, shuffle=False, collate_fn=lambda x:x)

    best_val_loss = np.inf
    best_model_param = None
    torch.manual_seed(42)

    criterion = nn.MSELoss(reduction='none')
    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=decay)

    train_losses = np.zeros((epochs, 3))
    valid_losses = np.zeros((epochs, 3))
    counter = 0 # for early stopping

    for epoch in range(epochs):
        train_loss = 0
        for i, batch in enumerate(train_loader):
            inputs, targets = tuple(zip(*batch)) # each is a tuple of tensors
            seq_lengths = list(map(len, inputs)) # get length of each sequence

            # Reformat to allow for variable sequence lengths with torch helpers
            padded = pad_sequence(inputs, batch_first=True, padding_value=pad_value)
            padded_packed = pack_padded_sequence(padded, seq_lengths, batch_first=True, enforce_sorted=False).float()
            padded_targets = pad_sequence(targets, batch_first=True, padding_value=pad_value)
            targets = torch.cat(targets).float()
            
            if torch.cuda.is_available():
                padded_packed = padded_packed.cuda()
                targets = targets.cuda()

            # Make predictions
            preds = model(padded_packed)

            # Unpad predictions based on target lengths
            flag = (padded_targets != pad_value)
            preds_res = preds[flag].reshape(-1, 3).split(seq_lengths)
            preds = torch.cat(preds_res).float()

            # Calculate loss
            loss = criterion(preds, targets)

            loss = loss.mean(axis=0)
            train_loss += loss

            loss = loss.mean()
            loss.backward() # back propagation, compute gradients
            optimizer.step() # apply gradients
            optimizer.zero_grad() # clear gradients for next train

        train_losses[epoch] = train_loss.cpu().detach().numpy()/(i+1)
        valid_losses[epoch] = evaluate(model, valid_loader, criterion, pad_value)
        print(f"Epoch: {epoch+1}, Train Loss: {train_losses[epoch].mean()}, Valid Loss: {valid_losses[epoch].mean()}")

        if valid_losses[epoch].mean() < best_val_loss:
            print('Saving Best Model')
            best_val_loss = valid_losses[epoch].mean()
            best_model_param = model.state_dict()
            counter = 0

        # early stopping
        if counter > 30: 
            train_losses = train_losses[:epoch+1]
            valid_losses = valid_losses[:epoch+1]
            break
        counter += 1

    if save:
        print(f'Writing best model parameter to {save_path}')
        torch.save(best_model_param, save_path)
            
    return model, train_losses, valid_losses


# In[42]:


save_path = 'models/GRU/gru_regressor'
model, train_losses, valid_losses = train_regression(save=True, save_path=save_path, batch_size=128, 
                                                     learning_rate=0.001)


# In[43]:


fig = plt.figure(figsize=(15,5))
ax = fig.add_subplot(1, 2, 1)
plt.plot(range(len(train_losses)), train_losses)
plt.title('Training Loss')
plt.legend(blood_types)
ax = fig.add_subplot(1,2, 2)
plt.plot(range(len(valid_losses)), valid_losses)
plt.title('Validation loss')
plt.legend(blood_types)
plt.show()


# In[44]:


# best result
pad_value=-999
save_path = 'models/GRU/gru_regressor'
model.load_state_dict(torch.load(save_path))

cols = pd.MultiIndex.from_product([['Train', 'Valid'], blood_types])
indices = pd.MultiIndex.from_product([[], []])
score_df = pd.DataFrame(index=indices, columns=cols)
score_df.loc[('GRU', 'RMSE'), :] = np.nan

train_loader = DataLoader(dataset=train_dataset, batch_size=len(train_dataset), shuffle=False, collate_fn=lambda x:x)
valid_loader = DataLoader(dataset=valid_dataset, batch_size=len(valid_dataset), shuffle=False, collate_fn=lambda x:x)
test_loader = DataLoader(dataset=test_dataset, batch_size=len(test_dataset), shuffle=False, collate_fn=lambda x:x)
    
    
for split, (X, Y, minmax), loader in [('Train', train, train_loader), 
                                      ('Valid', valid, valid_loader), 
                                      ('Test', test, test_loader)]:
    for i, batch in enumerate(loader):
        inputs, targets = tuple(zip(*batch))
        seq_lengths = list(map(len, inputs))
        padded = pad_sequence(inputs, batch_first=True, padding_value=pad_value)
        padded_packed = pack_padded_sequence(padded, seq_lengths, batch_first=True, enforce_sorted=False).float()
        padded_targets = pad_sequence(targets, batch_first=True, padding_value=pad_value)
        targets = torch.cat(targets).float()
        if torch.cuda.is_available():
            padded_packed = padded_packed.cuda()
            targets = targets.cuda()
        preds = model(padded_packed)
        flag = (padded_targets != pad_value)
        preds_res = preds[flag].reshape(-1, 3).split(seq_lengths)
        preds = torch.cat(preds_res).float()
    preds = preds.cpu().detach().numpy()
    targets = targets.cpu().detach().numpy()
    for idx, blood_type in enumerate(blood_types):
        scale = minmax[blood_type]
        y_true = targets[:, idx]*(scale.max()-scale.min())
        y_pred = preds[:, idx]*(scale.max()-scale.min())
        score_df.loc[('GRU', 'RMSE'), (split, blood_type)] =  mean_squared_error(y_true, y_pred, squared=False)


# In[45]:


score_df


# In[11]:


minmax[['neutrophil','hemoglobin', 'platelet']]


# In[ ]:


def gru_mse(hidden_size, hidden_layers, dropout, batch_size, learning_rate):
    # This is the target function to be optimized during hyperparameter tuning
    hidden_size = int(round(hidden_size))
    hidden_layers = int(round(hidden_layers))
    batch_size = int(round(batch_size))
    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, collate_fn=lambda x:x)
    valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, collate_fn=lambda x:x)
    model, train_losses, valid_losses = train_regression(batch_size=batch_size, learning_rate=learning_rate,
                                                         dropout=dropout, hidden_size=hidden_size, 
                                                         hidden_layers=hidden_layers)
    return valid_losses.mean()


# In[ ]:


gru_config = {'hidden_size': (10, 200), 
              'hidden_layers': (1, 10),
              'dropout': (0, 0.9),
              'batch_size': (16, 512),
              'learning_rate': (0.00001, 0.001)}


# In[ ]:


def get_bayesopt(target_function, config, init_points=2, n_iter=2):
    bo = BayesianOptimization(target_function, config, random_state=416)
    bo.maximize(init_points=init_points, n_iter=n_iter, acq='ei')
    return bo.max


# In[ ]:


gru_opt = get_bayesopt(gru_mse, gru_config)


# # Classification

# In[14]:


X_train, Y_train, minmax = train
X_valid, Y_valid, minmax = valid
X_test, Y_test, minmax = test


# In[15]:


n_min, n_max = minmax['neutrophil']
h_min, h_max = minmax['hemoglobin']
p_min, p_max = minmax['platelet']
neutrophil_threshold = (1.5 - n_min)/(n_max - n_min)
hemoglobin_threshold = (100 - h_min)/(h_max - h_min)
platelet_threshold = (75 - p_min)/(p_max - p_min)


# In[16]:


def regression_to_classification(target):
    """
    Convert regression labels (last blood count value) to 
    classification labels (if last blood count value is below corresponding threshold)
    """
    target[f'neutrophil < 1.5'] = target['target_neutrophil_value'] < neutrophil_threshold
    target[f'hemoglobin < 100'] = target['target_hemoglobin_value'] < hemoglobin_threshold
    target[f'platelet < 75'] = target['target_platelet_value'] < platelet_threshold
    target = target.drop(columns=[f'target_{blood_type}_value' for blood_type in blood_types])
    return target


# In[17]:


Y_train = regression_to_classification(Y_train)
Y_valid = regression_to_classification(Y_valid)
Y_test = regression_to_classification(Y_test)


# In[18]:


Y_train_distribution = pd.DataFrame([Y_train[col].value_counts() for col in Y_train.columns])
Y_valid_distribution = pd.DataFrame([Y_valid[col].value_counts() for col in Y_valid.columns])
Y_test_distribution = pd.DataFrame([Y_test[col].value_counts() for col in Y_test.columns])
Y_distribution = pd.concat([Y_train_distribution, Y_valid_distribution, Y_test_distribution], axis=1)
cols = pd.MultiIndex.from_product([['Train', 'Valid','Test'], ['True', 'False']])
Y_distribution.columns = cols
# Y_distribution.to_csv('models/classification_label_distribution.csv', index=False)
Y_distribution


# In[19]:


train_mapping = {}
for ikn, group in tqdm.tqdm(X_train.groupby('ikn')):
    group = group.drop(columns=['ikn'])
    train_mapping[ikn] = (group, Y_train.loc[group.index])

valid_mapping = {}
for ikn, group in tqdm.tqdm(X_valid.groupby('ikn')):
    group = group.drop(columns=['ikn'])
    valid_mapping[ikn] = (group, Y_valid.loc[group.index])
    
test_mapping = {}
for ikn, group in tqdm.tqdm(X_test.groupby('ikn')):
    group = group.drop(columns=['ikn'])
    test_mapping[ikn] = (group, Y_test.loc[group.index])


# In[20]:


train_dataset = seq_data(mapping=train_mapping, ids=X_train['ikn'].unique())
valid_dataset = seq_data(mapping=valid_mapping, ids=X_valid['ikn'].unique())
test_dataset = seq_data(mapping=test_mapping, ids=X_test['ikn'].unique())


# In[ ]:


def evaluate(model, loader, criterion, pad_value):
    total_loss = 0
    total_score = 0
    for i, batch in enumerate(loader):
        inputs, targets = tuple(zip(*batch))
        seq_lengths = list(map(len, inputs))
        padded = pad_sequence(inputs, batch_first=True, padding_value=pad_value)
        padded_packed = pack_padded_sequence(padded, seq_lengths, batch_first=True, enforce_sorted=False).float()
        padded_targets = pad_sequence(targets, batch_first=True, padding_value=pad_value)
        targets = torch.cat(targets).float()
        if torch.cuda.is_available():
            padded_packed = padded_packed.cuda()
            targets = targets.cuda()
        preds = model(padded_packed)
        flag = (padded_targets != pad_value)
        preds_res = preds[flag].reshape(-1, 3).split(seq_lengths)
        preds = torch.cat(preds_res).float()
        loss = criterion(preds, targets)
        loss = loss.mean(axis=0)
        total_loss += loss
        
        preds = preds > 0.5
        if torch.cuda.is_available():
            preds = preds.cpu().detach().numpy()
            targets = targets.cpu().detach().numpy()
        total_score += np.array([accuracy_score(targets[:, i], preds[:, i]) for i in range(3)])
    return total_loss.cpu().detach().numpy() / (i+1), total_score/(i+1)

def train_classification(batch_size=512, pad_value=-999, epochs=200, learning_rate=0.001, 
                hidden_size=20, hidden_layers=3, dropout=0.5, decay=0, save=False, save_path=None):
    
    n_features = len(X_train.columns) - 1 # -1 for ikn
    model = GRU_model(n_features=n_features, n_targets=3, hidden_size=hidden_size, hidden_layers=hidden_layers,
                      batch_size=batch_size, dropout=dropout, pad_value=pad_value)
    if torch.cuda.is_available():
        model.cuda()

    train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=False, collate_fn=lambda x:x)
    valid_loader = DataLoader(dataset=valid_dataset, batch_size=batch_size, shuffle=False, collate_fn=lambda x:x)

    best_val_loss = np.inf
    best_model_param = None
    torch.manual_seed(42)

    criterion = nn.MSELoss(reduction='none')
    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=decay)

    train_losses = np.zeros((epochs, 3))
    valid_losses = np.zeros((epochs, 3))
    train_scores = np.zeros((epochs, 3)) # acc score
    valid_scores = np.zeros((epochs, 3)) # acc score
    counter = 0 # for early stopping

    for epoch in range(epochs):
        train_loss = 0
        train_score = 0
        for i, batch in enumerate(train_loader):
            inputs, targets = tuple(zip(*batch)) # each is a tuple of tensors
            seq_lengths = list(map(len, inputs)) # get length of each sequence

            # Reformat to allow for variable sequence lengths with torch helpers
            padded = pad_sequence(inputs, batch_first=True, padding_value=pad_value)
            padded_packed = pack_padded_sequence(padded, seq_lengths, batch_first=True, enforce_sorted=False).float()
            padded_targets = pad_sequence(targets, batch_first=True, padding_value=pad_value)
            targets = torch.cat(targets).float()
            
            if torch.cuda.is_available():
                padded_packed = padded_packed.cuda()
                targets = targets.cuda()

            # Make predictions
            preds = model(padded_packed)

            # Unpad predictions based on target lengths
            flag = (padded_targets != pad_value)
            preds_res = preds[flag].reshape(-1, 3).split(seq_lengths)
            preds = torch.cat(preds_res).float()

            # Calculate loss
            loss = criterion(preds, targets)

            loss = loss.mean(axis=0)
            train_loss += loss
            
            preds = preds > 0.5
            if torch.cuda.is_available():
                preds = preds.cpu().detach().numpy()
                targets = targets.cpu().detach().numpy()
            train_score += np.array([accuracy_score(targets[:, i], preds[:, i]) for i in range(3)])

            loss = loss.mean()
            loss.backward() # back propagation, compute gradients
            optimizer.step() # apply gradients
            optimizer.zero_grad() # clear gradients for next train

        train_losses[epoch] = train_loss.cpu().detach().numpy()/(i+1)
        train_scores[epoch] = train_score/(i+1)
        valid_losses[epoch], valid_scores[epoch] = evaluate(model, valid_loader, criterion, pad_value)
        statement = f"Epoch: {epoch+1}, Train Loss: {np.round(train_losses[epoch].mean(),4)}, Valid Loss: {np.round(valid_losses[epoch].mean(),4)}, Train Accuracy: {np.round(train_scores[epoch].mean(), 4)}, Valid Accuracy: {np.round(valid_scores[epoch].mean(),4)}"
        print(statement)

        if valid_losses[epoch].mean() < best_val_loss:
            print('Saving Best Model')
            best_val_loss = valid_losses[epoch].mean()
            best_model_param = model.state_dict()
            counter = 0

        # early stopping
        if counter > 30: 
            train_losses = train_losses[:epoch+1]
            valid_losses = valid_losses[:epoch+1]
            train_scores = train_scores[:epoch+1]
            valid_scores = valid_scores[:epoch+1]
            break
        counter += 1

    if save:
        print(f'Writing best model parameter to {save_path}')
        torch.save(best_model_param, save_path)
            
    return model, train_losses, valid_losses, train_scores, valid_scores


# In[76]:


save_path = 'models/GRU/gru_classifier'
model, train_losses, valid_losses, train_scores, valid_scores = train_classification(save=True, save_path=save_path, 
                                                                                 batch_size=128, learning_rate=0.001)


# In[85]:


fig = plt.figure(figsize=(15,10))
ax = fig.add_subplot(2, 2, 1)
plt.plot(range(len(train_losses)), train_losses)
plt.title('Training Loss')
plt.legend(blood_types)
ax = fig.add_subplot(2,2, 2)
plt.plot(range(len(valid_losses)), valid_losses)
plt.title('Validation loss')
plt.legend(blood_types)
ax = fig.add_subplot(2, 2, 3)
plt.plot(range(len(train_scores)), train_scores)
plt.title('Training Accuracy')
plt.legend(blood_types)
ax = fig.add_subplot(2, 2, 4)
plt.plot(range(len(valid_scores)), valid_scores)
plt.title('Validation Accuracy')
plt.legend(blood_types)
plt.savefig('models/GRU/loss_and_acc.jpg')
plt.show()


# In[24]:


# best result
model = GRU_model(n_features=len(X_train.columns)-1, n_targets=3, hidden_size=20, hidden_layers=3,
                  batch_size=512, dropout=0.5, pad_value=-999)
model.cuda()

pad_value=-999
save_path = 'models/GRU/gru_classifier'
model.load_state_dict(torch.load(save_path))

cols = pd.MultiIndex.from_product([['Train', 'Valid'], blood_types])
indices = pd.MultiIndex.from_product([[], []])
score_df = pd.DataFrame(index=indices, columns=cols)
algorithm = 'GRU'
for metric in ['Acc', 'Precision', 'Recall', 'F1 Score', 'ROC AUC Score', 'AP Score']:
    score_df.loc[(algorithm, metric), :] = np.nan
   
train_loader = DataLoader(dataset=train_dataset, batch_size=len(train_dataset), shuffle=False, collate_fn=lambda x:x)
valid_loader = DataLoader(dataset=valid_dataset, batch_size=len(valid_dataset), shuffle=False, collate_fn=lambda x:x)
test_loader = DataLoader(dataset=test_dataset, batch_size=len(test_dataset), shuffle=False, collate_fn=lambda x:x)

for split, (X, Y, minmax), loader in [('Train', train, train_loader), 
                                      ('Valid', valid, valid_loader), 
                                      ('Test', test, test_loader)]:
    for i, batch in enumerate(loader):
        inputs, targets = tuple(zip(*batch))
        seq_lengths = list(map(len, inputs))
        padded = pad_sequence(inputs, batch_first=True, padding_value=pad_value)
        padded_packed = pack_padded_sequence(padded, seq_lengths, batch_first=True, enforce_sorted=False).float()
        padded_targets = pad_sequence(targets, batch_first=True, padding_value=pad_value)
        targets = torch.cat(targets).float()
        if torch.cuda.is_available():
            padded_packed = padded_packed.cuda()
            targets = targets.cuda()
        preds = model(padded_packed)
        flag = (padded_targets != pad_value)
        preds_res = preds[flag].reshape(-1, 3).split(seq_lengths)
        preds = torch.cat(preds_res).float()
    preds = preds.cpu().detach().numpy()
    targets = targets.cpu().detach().numpy()
    
    for idx, blood_type in enumerate(blood_types):
        Y_true = targets[:, idx]
        Y_pred_prob = preds[:, idx]
        Y_pred_bool = Y_pred_prob > 0.5
        report = classification_report(Y_true, Y_pred_bool, output_dict=True, zero_division=1)
        
        score_df.loc[(algorithm, 'Acc'), (split, blood_type)] = report['accuracy']
        score_df.loc[(algorithm, 'Precision'), (split, blood_type)] = report['0.0']['precision']
        score_df.loc[(algorithm, 'Recall'), (split, blood_type)] = report['0.0']['recall']
        score_df.loc[(algorithm, 'F1 Score'), (split, blood_type)] = report['0.0']['f1-score']
        score_df.loc[(algorithm, 'ROC AUC Score'), (split, blood_type)] = roc_auc_score(Y_true, Y_pred_prob)
        score_df.loc[(algorithm, 'AP Score'), (split, blood_type)] = average_precision_score(Y_true, Y_pred_prob)
        
        # confusion matrix
        if split == 'Valid':
            cm = confusion_matrix(Y_true, Y_pred_bool)
            cm = pd.DataFrame(cm, columns=['Predicted False', 'Predicted True'], index=['Actual False', 'Actual True'])
            print(f"\n############ {split} - {blood_type} #############")
            print(cm)


# In[31]:


score_df = pd.DataFrame(score_df.values.astype(float).round(4), index=score_df.index, columns=score_df.columns)
score_df.to_csv('models/GRU/classification_result.csv')


# In[32]:


score_df


# In[33]:


score_df.loc[[i for i in score_df.index if 'AUC' in i[1]]]


# In[ ]:




