#!/usr/bin/env python
# coding: utf-8
"""
========================================================================
Â© 2021 Institute for Clinical Evaluative Sciences. All rights reserved.

TERMS OF USE:
##Not for distribution.## This code and data is provided to the user solely for its own non-commercial use by individuals and/or not-for-profit corporations. User shall not distribute without express written permission from the Institute for Clinical Evaluative Sciences.

##Not-for-profit.## This code and data may not be used in connection with profit generating activities.

##No liability.## The Institute for Clinical Evaluative Sciences makes no warranty or representation regarding the fitness, quality or reliability of this code and data.

##No Support.## The Institute for Clinical Evaluative Sciences will not provide any technological, educational or informational support in connection with the use of this code and data.

##Warning.## By receiving this code and data, user accepts these terms, and uses the code and data, solely at its own risk.
========================================================================
"""
# In[1]:


import sys


# In[2]:


for i, p in enumerate(sys.path):
    sys.path[i] = sys.path[i].replace("/software/anaconda/3/", "/MY/DATA/.conda/envs/myenv/")


# In[3]:


import os
import tqdm
import itertools
import pandas as pd
import numpy as np
import multiprocessing as mp
import datetime as dt
import utilities as util
import matplotlib.pyplot as plt

from functools import partial
from collections import Counter, defaultdict


# In[4]:


chemo_df = pd.read_csv('data/chemo_processed.csv', dtype={'ikn': str})
chemo_df['visit_date'] = pd.to_datetime(chemo_df['visit_date'])
chemo_df['prev_visit'] = pd.to_datetime(chemo_df['prev_visit'])
chemo_df['chemo_interval'] = pd.to_timedelta(chemo_df['chemo_interval'])


# In[5]:


df = util.read_partially_reviewed_csv()
df = util.get_included_regimen(df)
cycle_lengths = df['cycle_length'].to_dict()
del df


# In[32]:


processes = 50 #  <-- srun -p high -c 50 --mem=96G --pty bash
 
# Manager to create a shared object
manager = mp.Manager()

# create global objects
shared_dict = manager.dict()


# In[7]:


def parallelize(filtered_chemo_df, worker, processes=16):
    # splits chemo_df into x number of partitions, where x is number of processes
    generator = np.array_split(filtered_chemo_df, processes)
    pool = mp.Pool(processes=processes)
    result = pool.map(worker, generator)
    pool.close()
    pool.join() # wait for all threads
    return result


# In[8]:


def sas_to_csv(name):
    chunks = pd.read_sas(f'ykaliwal/{name}.sas7bdat', chunksize=10**6)
    for i, chunk in tqdm.tqdm(enumerate(chunks)):
        # remove empty rows
        chunk = chunk[~chunk.iloc[:,1:].isnull().all(axis=1)]
    
        # write to csv
        header = True if i == 0 else False
        chunk.to_csv(f"data/{name}1.csv", header=header, mode='a', index=False)


# In[17]:


chemo_df.to_csv('data/chemo_processed2.csv', index=False)


# # esas

# In[ ]:


sas_to_csv('esas')


# In[ ]:


def filter_esas(chunk):
    chunk = chunk.rename(columns={'esas_value': 'severity', 'esas_resourcevalue': 'symptom'})
    # remove first two characters "b'" and last character "'"
    for col in ['ikn', 'severity', 'symptom']:
        chunk[col] = chunk[col].str[2:-1]
    chunk = chunk.drop(columns=['Unnamed: 0'])
    chunk['surveydate'] = pd.to_datetime(chunk['surveydate'])
    chunk['severity'] = chunk['severity'].astype(int)
    
    # filter patients not in chemo_df
    chunk = chunk[chunk['ikn'].isin(chemo_df['ikn'])]
    
    # sort by date
    chunk = chunk.sort_values(by='surveydate')
    
    # filter out patients not in chunk
    filtered_chemo_df = chemo_df[chemo_df['ikn'].isin(chunk['ikn'])]
    
    return chunk, filtered_chemo_df


# In[11]:


def worker(partition):
    esas = shared_dict['esas_chunk']

    result = []
    for ikn, group in partition.groupby('ikn'):
        esas_specific_ikn = esas[esas['ikn'] == ikn]
        for idx, chemo_row in group.iterrows():
            visit_date = chemo_row['visit_date']
            esas_most_recent = esas_specific_ikn[esas_specific_ikn['surveydate'] < visit_date]
            if not esas_most_recent.empty:
                symptoms = list(esas_most_recent['symptom'].unique())
                for symptom in symptoms:
                    esas_specific_symptom = esas_most_recent[esas_most_recent['symptom'] == symptom]

                    # last item is always the last observed grade (we've already sorted by date)
                    result.append((idx, symptom, esas_specific_symptom['severity'].iloc[-1]))
    return result


# In[12]:


chunks = pd.read_csv('data/esas1.csv', chunksize=10**6) # chunksize=10**6, i=44, 1:05
result = [] #np.load('data/checkpoint/esas_features_chunk0.npy')
for i, chunk in tqdm.tqdm(enumerate(chunks), total=44):
    # if i < 0: continue
    chunk, filtered_chemo_df = filter_esas(chunk)
    
    # get results
    shared_dict['esas_chunk'] = chunk
    chunk_result = parallelize(filtered_chemo_df, worker)

    chunk_result = list(itertools.chain(*chunk_result))
    result += chunk_result
    if i != 0:
        os.remove(f'data/checkpoint/esas_features_chunk{i-1}.npy')
    np.save(f'data/checkpoint/esas_features_chunk{i}.npy', result)
np.save('data/esas_features.npy', result)


# In[16]:


esas_features = np.load('data/esas_features.npy')

symptoms = set(esas_features[:, 1]) 
# clean up - remove Activities & Function as they only have 26 samples
symptoms.remove('Activities & Function:')
result = {symptom: [] for symptom in symptoms}
esas_idx_mapping = {}

# fill out the esas_idx_mapping
# TODO CLEAN UP THE DUPLICATES USING SURVEYDATE - xxxx out of xxxxxxx
for idx,symptom,severity in tqdm.tqdm(esas_features):
    if symptom in esas_idx_mapping:
        esas_idx_mapping[symptom][idx] = severity
    else:
        esas_idx_mapping[symptom] = {idx: severity}
        
# fill out the results
for i in tqdm.tqdm(range(0, len(chemo_df))):
    for symptom in symptoms:
        if str(i) in esas_idx_mapping[symptom]:
            result[symptom].append(esas_idx_mapping[symptom][str(i)])
        else:
            result[symptom].append(np.nan)

# mean impute the missing data
esas = pd.DataFrame(result)
esas = esas.fillna(esas.mode().iloc[0])

# put esas features in chemo_df
chemo_df = pd.concat([chemo_df, esas], axis=1)


# # ecog

# In[ ]:


sas_to_csv('ecog')


# In[9]:


ecog = pd.read_csv('data/ecog1.csv')

# organize and format columns
ecog = ecog.drop(columns=['ecog_resourcevalue'])
ecog = ecog.rename(columns={'ecog_value': 'ecog_grade'})
# remove first two characters "b'" and last character "'"
for col in ['ikn', 'ecog_grade']:
    ecog[col] = ecog[col].str[2:-1]
ecog['ecog_grade'] = ecog['ecog_grade'].astype(int)
ecog['surveydate'] = pd.to_datetime(ecog['surveydate'])

# filter out patients not in chemo_df
ecog = ecog[ecog['ikn'].isin(chemo_df['ikn'])]

# sort by date
ecog = ecog.sort_values(by='surveydate')

# filter out patients not in ecog
filtered_chemo_df = chemo_df[chemo_df['ikn'].isin(ecog['ikn'])]


# In[10]:


shared_dict['ecog'] = ecog
def worker(partition):
    ecog = shared_dict['ecog']
    result = []
    for ikn, group in tqdm.tqdm(partition.groupby('ikn'), position=0):
        ecog_specific_ikn = ecog[ecog['ikn'] == ikn]
        for idx, chemo_row in group.iterrows():
            visit_date = chemo_row['visit_date']
            ecog_most_recent = ecog_specific_ikn[ecog_specific_ikn['surveydate'] < visit_date]
            if not ecog_most_recent.empty:
                # last item is always the last observed grade (we've already sorted by date)
                result.append((idx, ecog_most_recent['ecog_grade'].iloc[-1]))
    return result


# In[11]:


# get results
result = parallelize(filtered_chemo_df, worker)
data_list = list(itertools.chain(*result))
np.save('data/ecog_features.npy', data_list)


# In[15]:


ecog_features = np.load('data/ecog_features.npy')
idx_ecog_mapping = {idx: ecog_grade for idx, ecog_grade in ecog_features}

# fill out result
result = []
for i in tqdm.tqdm(range(0, len(chemo_df))):
    if i in idx_ecog_mapping:
        result.append(idx_ecog_mapping[i])
    else:
        result.append(np.nan)
        
# put ecog grade in chemo_df
chemo_df['ecog_grade'] = result

# mean impute missing values
mode = chemo_df['ecog_grade'].mode()[0]
chemo_df['ecog_grade'] = chemo_df['ecog_grade'].fillna(mode)


# # dad_transfusion (hospitalization visit)

# In[26]:


sas_to_csv('dad_transfusion')


# In[9]:


def dad_filter(chunk):
    # remove first two characters "b'" and last character "'"
    for col in ['ikn', 'source', 'btany', 'btplate', 'btredbc']:
        chunk[col] = chunk[col].str[2:-1]
    chunk['admdate'] = pd.to_datetime(chunk['admdate'])
    chunk['ddate'] = pd.to_datetime(chunk['ddate'])
    
    # filter patients not in chemo_df
    chunk = chunk[chunk['ikn'].isin(chemo_df['ikn'])]
    
    # filter rows where no transfusions occured
    chunk = chunk[(chunk['btany'] == 'Y') | (chunk['btany'] == '1')]
    
    # get only the dates of transfusions
    chunk = chunk[['admdate', 'ddate', 'ikn']]
    
    # sort by date
    chunk = chunk.sort_values(by='ddate')
    
    # filter out patients not in dad_transfusion
    filtered_chemo_df = chemo_df[chemo_df['ikn'].isin(chunk['ikn'])]
    
    return chunk, filtered_chemo_df


# In[ ]:


def worker(partition):
    dad = shared_dict['dad_chunk']

    result = set()
    for ikn, group in partition.groupby('ikn'):
        dad_specific_ikn = dad[dad['ikn'] == ikn]
        for i, dad_row in dad_specific_ikn.iterrows():
            ddate = dad_row['ddate']
            admdate = dad_row['admdate']
            mask = ((ddate <= group['visit_date']) & (ddate >= group['prev_visit']) | 
                    (admdate <= group['visit_date']) & (admdate >= group['prev_visit']))
            result.union(group[mask].index)
    return result


# In[11]:


chunks = pd.read_csv('data/dad_transfusion1.csv', chunksize=10**6) # chunksize=10**6, i=7, 0:12
result = set()
for i, chunk in tqdm.tqdm(enumerate(chunks), total=7):
    chunk, filtered_chemo_df = dad_filter(chunk)
    shared_dict['dad_chunk'] = chunk
    chunk_result = parallelize(filtered_chemo_df, worker)
    chunk_result = list(itertools.chain(*chunk_result))
    result.union(chunk_result)


# In[13]:


result


# # nacrs transfusion (ER Visit)

# In[27]:


sas_to_csv('nacrs_transfusion')


# In[33]:


def nacrs_filter(chunk):
    # remove first two characters "b'" and last character "'"
    for col in ['ikn', 'btany', 'btplate', 'btredbc']:
        chunk[col] = chunk[col].str[2:-1]
    chunk['regdate'] = pd.to_datetime(chunk['regdate'])
    
    # filter patients not in chemo_df
    chunk = chunk[chunk['ikn'].isin(chemo_df['ikn'])]
    
    # filter rows where no transfusions occured
    chunk = chunk[(chunk['btany'] == 'Y')]
    
    # get only the dates of transfusions
    chunk = chunk[['regdate', 'ikn']]
    
    # sort by date
    chunk = chunk.sort_values(by='regdate')
    
    # filter out patients not in dad_transfusion
    filtered_chemo_df = chemo_df[chemo_df['ikn'].isin(chunk['ikn'])]
    
    return chunk, filtered_chemo_df


# In[34]:


def worker(partition):
    nacrs = shared_dict['nacrs_chunk']

    result = set()
    for ikn, group in partition.groupby('ikn'):
        nacrs_specific_ikn = nacrs[nacrs['ikn'] == ikn]
        for i, nacrs_row in nacrs_specific_ikn.iterrows():
            regdate = nacrs_row['regdate']
            mask = (regdate <= group['visit_date']) & (regdate >= group['prev_visit'])
            result.union(group[mask].index)
    return result


# In[35]:


chunks = pd.read_csv('data/nacrs_transfusion1.csv', chunksize=10**6) # chunksize=10**6, i=9, 0:05
result = set()
for i, chunk in tqdm.tqdm(enumerate(chunks), total=9):
    chunk, filtered_chemo_df = nacrs_filter(chunk)
    shared_dict['nacrs_chunk'] = chunk
    chunk_result = parallelize(filtered_chemo_df, worker)
    chunk_result = list(itertools.chain(*chunk_result))
    result.union(chunk_result)


# In[36]:


result


# # odb_growth factors

# In[178]:


sas_to_csv('odb_growth_factors')


# In[139]:


odb = pd.read_csv('data/odb_growth_factors1.csv')

# remove first two characters "b'" and last character "'"
for col in ['ikn']:
    odb[col] = odb[col].str[2:-1]
odb['servdate'] = pd.to_datetime(odb['servdate'])

# filter out patients not in chemo_df
odb = odb[odb['ikn'].isin(chemo_df['ikn'])]

# sort by date
odb = odb.sort_values(by='servdate')

# filter out patients not in ecog
filtered_chemo_df = chemo_df[chemo_df['ikn'].isin(odb['ikn'])]


# In[142]:


shared_dict['odb'] = odb
def worker(partition):
    odb = shared_dict['odb']
    result = set()
    for ikn, group in partition.groupby('ikn'):
        odb_specific_ikn = odb[odb['ikn'] == ikn]
        for i, odb_row in odb_specific_ikn.iterrows():
            servdate = odb_row['servdate']
            mask = (servdate <= group['visit_date']) & (servdate >= group['prev_visit'])
            result.union(group[mask].index)
    return result


# In[143]:


result = parallelize(filtered_chemo_df, worker)
result = list(itertools.chain(*result))


# In[144]:


result


# # olis blood count

# In[8]:


sas_to_csv('olis_blood_count')


# In[ ]:


def olis_filter1(chunk):
    # keep only selected columns
    olis_cols = ['ikn', 'ObservationCode', 'ObservationDateTime', 'ObservationReleaseTS', 'ReferenceRange',
                 'Units','Value_recommended_d']
    chunk = chunk[olis_cols]
    # remove first two characters "b'" and last character "'"
    for col in ['ikn', 'ObservationCode', 'ReferenceRange', 'Units']:
        chunk[col] = chunk[col].str[2:-1]
    
    # filter patients not in chemo_df
    chunk = chunk[chunk['ikn'].isin(chemo_df['ikn'])]
    
    # filter platelet, neutrophil, and hemoglobin
    chunk = chunk[~chunk['ObservationCode'].isin(['718-7', '751-8', '777-3'])]
    
    # rename value recommended d to value
    chunk = chunk.rename(columns={'Value_recommended_d': 'value'})
    
    return chunk


# In[ ]:


chunks = pd.read_csv('data/olis_blood_count1.csv', chunksize=10**6) # chunksize=10**6, i=331, 19:09
for i, chunk in tqdm.tqdm(enumerate(chunks), total=331):
    chunk = olis_filter1(chunk)
    # write to csv
    header = True if i == 0 else False
    # chunk.to_csv(f"data/olis_blood_count2.csv", header=header, mode='a', index=False)


# In[9]:


keep_blood_types = {'4544-3': 'hematocrit',
                    '6690-2': 'leukocytes',
                    '787-2': 'erythrocyte_mean_corpuscular_volume',
                    '789-8': 'erythrocytes',
                    '788-0': 'erythrocyte_distribution_width',
                    '785-6': 'erythrocyte_mean_corpuscular_hemoglobin',
                    '786-4': 'erythrocyte_mean_corpuscular_hemoglobin_concentration',
                    '731-0': 'lymphocytes',
                    '711-2': 'eosinophils',
                    '704-7': 'basophils',
                    '742-7': 'monocytes',
                    '32623-1': 'platelet_mean_volume'}

def olis_filter2(chunk):
    # Convert string column into timestamp column
    chunk['ObservationDateTime'] = pd.to_datetime(chunk['ObservationDateTime'])
    chunk['ObservationDateTime'] = chunk['ObservationDateTime'].dt.floor('D') # keep only the date, not time
    chunk['ObservationReleaseTS'] = pd.to_datetime(chunk['ObservationReleaseTS'])
    
    # Filter rows with excluded blood types
    chunk = chunk[chunk['ObservationCode'].isin(keep_blood_types)]
    
    # Filter rows with blood count null values
    chunk = chunk[~chunk['value'].isnull()]

    # Remove duplicate rows
    subset = ['ikn','ObservationCode', 'ObservationDateTime', 'value']
    chunk = chunk.drop_duplicates(subset=subset) 
    
    # If only the patient id, blood, and observation timestamp are duplicated (NOT the blood count value), 
    # keep the most recently RELEASED row
    chunk = chunk.sort_values(by='ObservationReleaseTS')
    subset = ['ikn','ObservationCode', 'ObservationDateTime']
    chunk = chunk.drop_duplicates(subset=subset, keep='last')
    
    # only keep rows where patient ids exist in olis chunk
    filtered_chemo_df = chemo_df[chemo_df['ikn'].isin(chunk['ikn'])]
    
    return filtered_chemo_df, chunk


# In[10]:


def worker(partition):
    olis = shared_dict['olis_chunk']
    result = []
    for ikn, chemo_group in partition.groupby('ikn'):
        olis_subset = olis[olis['ikn'] == ikn]
        for chemo_idx, chemo_row in chemo_group.iterrows():
            
            # see if there any blood count data within the target dates
            earliest_date = chemo_row['prev_visit'] - pd.Timedelta('5 days')
            # set limit to 28 days after chemo administration or the day of next chemo administration, 
            # whichever comes first
            latest_date = min(chemo_row['visit_date'], chemo_row['prev_visit'] + pd.Timedelta('28 days'))
            tmp = olis_subset[(earliest_date <= olis_subset['ObservationDateTime']) & 
                              (latest_date >= olis_subset['ObservationDateTime'])]
            
            # loop through the blood count data
            for blood_idx, blood_row in tmp.iterrows():
                blood_type = blood_row['ObservationCode']
                blood_count = blood_row['value']
                obs_date = blood_row['ObservationDateTime']
                days_after_chemo = (obs_date - chemo_row['prev_visit']).days
                # place onto result
                result.append((blood_type, chemo_idx, days_after_chemo, blood_count))
            
    return result


# In[11]:


chunks = pd.read_csv('data/olis_blood_count2.csv', dtype={'ikn':str}, chunksize=10**6) # chunksize=10**6, i=62, 0:51
result = [] # np.load('data/checkpoint/olis_blood_count_chunk5.npy').tolist()
for i, chunk in tqdm.tqdm(enumerate(chunks), total=62):
    # if i < 6: continue
    filtered_chemo_df, chunk = olis_filter2(chunk)
    shared_dict['olis_chunk'] = chunk
    chunk_result = parallelize(filtered_chemo_df, worker, processes=processes)
    chunk_result = list(itertools.chain(*chunk_result))
    result += chunk_result
    if i != 0:
        os.remove(f'data/checkpoint/olis_blood_count_chunk{i-1}.npy')
    np.save(f'data/checkpoint/olis_blood_count_chunk{i}.npy', result)
    print(f'OLIS blood count chunk {i} completed: size of result', len(result))
np.save('data/olis_blood_count.npy', result)


# In[12]:


olis_blood_count = np.load('data/olis_blood_count.npy') # all the ints are converted to strings
df = pd.DataFrame(olis_blood_count, columns=['blood_type', 'chemo_idx', 'days_after_chemo', 'blood_count'])
mapping = {blood_type: pd.DataFrame(index=chemo_df.index, columns=range(-5,29)) for blood_type in keep_blood_types}


# In[13]:


# fill up the blood count dataframes for each blood type
for blood_type, blood_group in tqdm.tqdm(df.groupby('blood_type')):
    for day, day_group in blood_group.groupby('days_after_chemo'):
        # print(f'Blood Type: {blood_type}, Days After Chemo: {day}, Number of Blood Samples: {len(day_group)}')
        chemo_indices = day_group['chemo_idx'].values.astype(int)
        blood_count_values = day_group['blood_count'].values.astype(float)
        mapping[blood_type].loc[chemo_indices, int(day)] = blood_count_values


# In[14]:


print('Number of rows for chemo_df:', len(chemo_df))
get_days_since_mes = lambda row, cycle: cycle - (np.where(~np.isnan(row))[0][-1] - 5)# adjust for -5 days
for blood_type, df in mapping.items():
    df['regimen'] = chemo_df['regimen']
    
    for regimen, group in df.groupby('regimen'):
        # set blood count measurements after regiment's respective cycle length minus 2 days to null
        cycle_length = int(cycle_lengths[regimen])
        df.loc[group.index, range(cycle_length-2, 29)] = np.nan
    
    # only keep rows that have at least 1 blood count measures
    df = df[(~df[range(-5,29)].isnull()).sum(axis=1) >= 1]
    
    values = df[range(-5,29)].values.astype(float)
    regimen_cycles = df['regimen'].map(cycle_lengths).astype(float).tolist()
    
    # days since prev observed measurement
    days_since = [get_days_since_mes(row, regimen_cycles[i]) for i, row in enumerate(values)]
    
    # get prev observed blood count measurement
    values = [row[~np.isnan(row)] for row in values] # get all non nan values from each row
    values = np.array([row[-1] for row in values]) # keep only the last value 
    
    blood_name = keep_blood_types[blood_type]
    chemo_df.loc[df.index, f'prev_{blood_name}_count'] = values
    chemo_df.loc[df.index, f'days_since_prev_{blood_name}_count'] = days_since
    print(f'Number of non-missing rows for {blood_name}:', 
          len(chemo_df[~chemo_df[f'prev_{blood_name}_count'].isnull()]))
del mapping


# # Scratch Notes

# ## display the csvs

# In[5]:


odb = pd.read_csv('data/odb_growth_factors1.csv')
odb


# In[25]:


chunks = pd.read_csv('data/esas1.csv', chunksize=10**6)
for i, chunk in tqdm.tqdm(enumerate(chunks), total=44):
    break
chunk


# In[15]:


ecog = pd.read_csv('data/ecog1.csv')
ecog


# In[24]:


chunks = pd.read_csv('data/dad_transfusion1.csv', chunksize=10**6) # chunksize=10**6, i=7, 0:12
for i, chunk in tqdm.tqdm(enumerate(chunks), total=7):
    break
chunk


# In[22]:


chunks = pd.read_csv('data/nacrs_transfusion1.csv', chunksize=10**6) # chunksize=10**6, i=9, 0:05
for i, chunk in tqdm.tqdm(enumerate(chunks), total=9):
    break


# In[23]:


chunk


# In[58]:


chunks = pd.read_csv('data/olis_blood_count1.csv', chunksize=10**6) # chunksize=10**6, i=331, 19:09
for i, chunk in tqdm.tqdm(enumerate(chunks), total=331):
    break
chunk


# ## Most Frequent Blood Type

# In[ ]:


'777-3': platelet    -  Platelets:NCnc:Pt:Bld:Qn:Automated Count
'751-8': neutrophil  -  Neutrophils:NCnc:Pt:Bld:Qn:Automated Count
'718-7': hemoglobin  -  Hemoglobin:MCnc:Pt:Bld:Qn

'20509-6': Hemoglobin:MCnc:Pt:Bld:Qn:Calculated
'26499-4': Neutrophils:NCnc:Pt:Bld:Qn
'753-4': Neutrophils:NCnc:Pt:Bld:Qn:Manual Count
'26515-7': Platelets:NCnc:Pt:Bld:Qn
'13056-7': Platelet:NCnc:Pt:Plas:Qn:Automated count   

'14196-0': Reticulocytes:NCnc:Pt:Rbc:Qn
'20570-8': Hematocrit:VFr:Pt:Bld:Qn
'26453-1': Erythrocytes:NCnc:Pt:Bld:Qn
'26444-0': Basophils:NCnc:Pt:Bld:Qn
'26449-9': Eosinophils:NCnc:Pt:Bld:Qn
'26464-8': Leukocytes:NCnc:Pt:Bld:Qn
'26474-7': Lymphocytes:NCnc:Pt:Bld:Qn
'26484-6': Monocytes:NCnc:Pt:Bld:Qn                                 

'21000-5': ERYTHROCYTE DISTRIBUTION WIDTH:ENTVOL:PT:RBC:QN:AUTOMATED COUNT
'30384-2': ERYTHROCYTE DISTRIBUTION WIDTH:ENTVOL:PT:RBC:QN
'30385-9': Erythrocyte distribution width:Ratio:Pt:RBC:Qn
'788-0': Erythrocyte distribution width:Ratio:Pt:RBC:Qn:Automated Count
'30428-7': Erythrocyte mean corpuscular volume:EntVol:Pt:RBC:Qn
'787-2': Erythrocyte mean corpuscular volume:EntVol:Pt:RBC:Qn:Automated Count
'28539-5': Erythrocyte mean corpuscular hemoglobin:EntMass:Pt:RBC:Qn
'785-6': Erythrocyte mean corpuscular hemoglobin:EntMass:Pt:RBC:Qn:Automated Count
'28540-3': Erythrocyte mean corpuscular hemoglobin concentration:MCnc:Pt:RBC:Qn
'786-4': Erythrocyte mean corpuscular hemoglobin concentration:MCnc:Pt:RBC:Qn:Automated Count

'28542-9': Platelet Mean Volume:ENTVOL:PT:Bld:Qn
'32623-1': Platelet Mean Volume:ENVOL:PT:Bld:Qn:Automated Count

'705-4': Basophils:NCnc:Pt:Bld:Qn:Manual Count
'712-0': Eosinophils:NCnc:Pt:Bld:Qn:Manual Count
'732-8': Lymphocytes:NCnc:Pt:Bld:Qn:Manual Count
'743-5': Monocytes:NCnc:Pt:Bld:Qn:Manual Count
'790-6': Erythrocytes:NCnc:Pt:Bld:Qn:Manual Count
'804-5': Leukocytes:NCnc:Pt:Bld:Qn:Manual Count

'4544-3': Hematocrit:VFr:Pt:Bld:Qn:Automated Count
'6690-2': Leukocytes:NCnc:Pt:Bld:Qn:Automated Count
'704-7': Basophils:NCnc:Pt:Bld:Qn:Automated Count
'711-2': Eosinophils:NCnc:Pt:Bld:Qn:Automated Count
'71833-8': Hematocrit:VFr.DF:Pt:Bld:Qn:Automated Count
'731-0': Lymphocytes:NCnc:Pt:Bld:Qn:Automated Count
'742-7': Monocytes:NCnc:Pt:Bld:Qn:Automated Count
'789-8': Erythrocytes:NCnc:Pt:Bld:Qn:Automated Count


# In[ ]:


chunks = pd.read_csv('data/olis_blood_count2.csv', chunksize=10**6) # chunksize=10**6, i=62, 0:51
freq_blood_type = Counter({})
for i, chunk in tqdm.tqdm(enumerate(chunks), total=62):
    freq_blood_type += Counter(dict(chunk['ObservationCode'].value_counts()))


# In[61]:


freq_blood_type.most_common()


# ## dad transfusion does not overlap

# In[21]:


dad = chunk
partition = filtered_chemo_df
result = set()
for ikn, group in partition.groupby('ikn'):
    dad_specific_ikn = dad[dad['ikn'] == ikn]
    for i, dad_row in dad_specific_ikn.iterrows():
        ddate = dad_row['ddate']
        admdate = dad_row['admdate']
        mask = ((ddate <= group['visit_date']) & (ddate >= group['prev_visit']) | 
                (admdate <= group['visit_date']) & (admdate >= group['prev_visit']))
        print(group[['visit_date', 'prev_visit']])
        print(ddate, admdate)
        result.union(group[mask].index)

